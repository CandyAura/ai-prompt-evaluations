# ai-prompt-evaluations
Comparing LLM responses across ChatGPT, Pi, and Gemini from a UX perspective.
# AI Prompt Evaluation – UX-Focused Comparison of LLM Responses

This project analyzes how three large language models—ChatGPT (OpenAI), Pi (Inflection AI), and Gemini (Google) respond to a real world, emotionally nuanced prompt from a user experience (UX) perspective. The goal is to evaluate communication tone, empathy, and clarity across systems that are increasingly used in support, coaching, and dialogue based applications.

## Prompt

> "How can someone set healthy boundaries without damaging their relationships?"

## Objective

To compare the conversational style, emotional intelligence, and usability of three LLMs when responding to a sensitive interpersonal question. The focus is on how each model supports user needs through content, tone, and structure.

## Evaluation Criteria

- Empathy and tone
- Actionable guidance
- Clarity and structure
- Personalization and follow up
- Alignment with UX writing and chatbot design principles

## Models Evaluated

- ChatGPT – OpenAI
- Pi – Inflection AI
- Gemini – Google

## Evaluation Summary

| Category           | ChatGPT        | Pi               | Gemini         |
|--------------------|----------------|------------------|----------------|
| Empathy            | Moderate       | High             | Low            |
| Tone               | Professional   | Human-like       | Clinical       |
| Actionable Advice  | Strong         | Light            | Structured     |
| Personalization    | Adaptive       | High             | Minimal        |
| UX Alignment       | High           | Naturalistic     | Formal         |

## Key Insights

- **Pi** was the most emotionally responsive, using a conversational tone and empathetic phrasing, but gave limited practical steps.
- **ChatGPT** offered the most balanced response: emotionally aware, structured, and full of practical suggestions.
- **Gemini** emphasized research backed structure and citations, but lacked warmth and direct engagement.

## Repository Structure

- `prompts/` – Contains the original prompt and full LLM responses
- `analysis/` – UX evaluation notes and comparison rationale
- `screenshots/` – (Optional) Visual captures of responses in each interface

## Future Work

- Extend analysis to additional prompts involving emotional intelligence, conflict resolution, or goal setting
- Evaluate multi turn dialogue UX with LLMs
- Compare newer LLM versions for consistency and alignment with ethical communication goals
